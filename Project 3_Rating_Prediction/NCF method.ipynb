{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Setting random seeds to replicate results easily\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4558f",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "\n",
    "We need a reliable way to evaluate the performance of recommendation algorithms. RMSE is one of the popular metrics to estimate how good the recommendation algorithm is. Since RMSE is measuring the prediction errors, the smaller error that the model can achieve, the better performance it is, and vice versa.\n",
    "\n",
    "$$RMSE=\\sqrt{\\sum_{i=1}^n\\frac{(\\hat{y}_i - y_i)^2}{N}}$$\n",
    "\n",
    "$\\hat{y}_i$: The predicted answer of sample $i$\n",
    "\n",
    "$y$: The ground truth answer of sample $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4875920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, actual):\n",
    "    '''\n",
    "    params:\n",
    "        pred <np.array>: an array containing all predicted ratings\n",
    "        actual <np.array>: an array containing all ground truth ratings\n",
    "\n",
    "    return:\n",
    "        a scalar whose value is the rmse\n",
    "    '''\n",
    "    # Ignore ratings with value zero.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return np.sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979452d",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF) Model Implementation\n",
    "\n",
    "Here we implement two instantiations of NCF model. \n",
    "\n",
    "The first instantiation computes the recommendation score (e.g., ratings) between a pair of user and item using dot product of their embeddings, which is equivalent to matrix factorization model for recommendation.\n",
    "\n",
    "The second instantiation concatenates the user's and item's embeddings, then feed the the concatenated vector into a MLP to calculate the recommendation score. Adoption of MLP equips the model with high flexibility and non-linearity to effectively learn the interaction between user and item latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ncf_model(n_users, n_items, embed_size, dropout='0.15', output_layer='dot'):\n",
    "    '''\n",
    "    params:\n",
    "        n_users <int>: The number of user embedding vectors\n",
    "        n_items <int>: The number of item embedding vectors\n",
    "        embed_size <int>: The dimension of each embedding vector\n",
    "        output_layer <str>: Indicates the instantiation of NCF to use, available options are either 'dot' or 'mlp'\n",
    "\n",
    "    return:\n",
    "        a keras Model object for the constructed ncf model \n",
    "    '''\n",
    "    # Get the users and items input\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "\n",
    "    # Get the embeddings of users and items\n",
    "    user_emb = Embedding(output_dim=embed_size, input_dim=n_users, input_length=1)(user_input)\n",
    "    user_emb = Reshape((embed_size,))(user_emb)\n",
    "    item_emb = Embedding(output_dim=embed_size, input_dim=n_items, input_length=1)(item_input)\n",
    "    item_emb = Reshape((embed_size,))(item_emb)\n",
    "\n",
    "\n",
    "    if output_layer == 'dot':\n",
    "        # Compute the dot product of users' and items' embeddings as the model output\n",
    "        model_output = Dot(axes=1)([user_emb, item_emb])\n",
    "\n",
    "    elif output_layer == 'mlp':\n",
    "        # Concatenate the users' and items' embeddings as the input of MLP\n",
    "        mlp_input = Concatenate()([user_emb, item_emb])\n",
    "\n",
    "        # First fully-connected layer\n",
    "        dense_1 = Dense(64, activation='relu', kernel_regularizer='l2')(mlp_input)\n",
    "        dense_1_dp = Dropout(dropout)(dense_1)\n",
    "\n",
    "        # Second fully-connected layer\n",
    "        dense_2 = Dense(32, activation='relu', kernel_regularizer='l2')(dense_1_dp)\n",
    "        dense_2_dp = Dropout(dropout)(dense_2)\n",
    "\n",
    "        # Final fully-connected layer to compute model output\n",
    "        model_output = Dense(1)(dense_2_dp)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ncf_combine_model(n_users, n_items, embed_size, output_layer='dot'):\n",
    "    '''\n",
    "    params:\n",
    "        n_users <int>: The number of user embedding vectors\n",
    "        n_items <int>: The number of item embedding vectors\n",
    "        embed_size <int>: The dimension of each embedding vector\n",
    "        output_layer <str>: Indicates the instantiation of NCF to use, available options are either 'dot' or 'mlp'\n",
    "\n",
    "    return:\n",
    "        a keras Model object for the constructed ncf model \n",
    "    '''\n",
    "    # Get the users and items input\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "\n",
    "    # Get the embeddings of users and items\n",
    "    user_emb = Embedding(output_dim=embed_size, input_dim=n_users, input_length=1)(user_input)\n",
    "    user_emb = Reshape((embed_size,))(user_emb)\n",
    "    item_emb = Embedding(output_dim=embed_size, input_dim=n_items, input_length=1)(item_input)\n",
    "    item_emb = Reshape((embed_size,))(item_emb)\n",
    "\n",
    "    model_output = Dot(axes=1)([user_emb, item_emb])\n",
    "\n",
    "    mlp_input = Concatenate()([user_emb, item_emb])\n",
    "\n",
    "    # First fully-connected layer\n",
    "    dense_1 = Dense(64, activation='relu', kernel_regularizer='l2')(mlp_input)\n",
    "    dense_1_dp = Dropout(0.0)(dense_1)\n",
    "\n",
    "    # Second fully-connected layer\n",
    "    dense_2 = Dense(32, activation='relu', kernel_regularizer='l2')(dense_1_dp)\n",
    "    dense_2_dp = Dropout(0.0)(dense_2)\n",
    "\n",
    "    \n",
    "    NMFLayer = Concatenate()([model_output, dense_2_dp])\n",
    "    NMFOutput = Dense(1, activation='relu')(NMFLayer)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=NMFOutput)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee02b9e",
   "metadata": {},
   "source": [
    "# Ratings Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f5b81",
   "metadata": {},
   "source": [
    "### Loading training and validation rating table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/review.csv\")\n",
    "val_df = pd.read_csv(\"data/validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c22ad",
   "metadata": {},
   "source": [
    "### Building two dictionaries to map original user ids and item ids into corresponding indices in respective embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique set of all user ids and set of all business ids in train set\n",
    "user_set = list(tr_df.ReviewerID.unique())\n",
    "business_set = list(tr_df.ProductID.unique())\n",
    "\n",
    "# Build user vocabulary\n",
    "user_vocab = dict(zip(user_set, range(1, len(user_set) + 1)))\n",
    "\n",
    "# Reserve the first row of the embedding matrix for users unseen in the training set\n",
    "user_vocab['unk'] = 0 \n",
    "n_users = len(user_vocab)\n",
    "\n",
    "# Build business vocabulary\n",
    "business_vocab = dict(zip(business_set, range(1, len(business_set) + 1)))\n",
    "# Reserve the first row of the embedding matrix for businesses unseen in the training set\n",
    "business_vocab['unk'] = 0\n",
    "n_items = len(business_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccd4c5",
   "metadata": {},
   "source": [
    "### Replacing the original user and item ids in train and valdiation set with indices in embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming user_id into a number by the user_vocab dictionary, and\n",
    "# transforming business_id into a number by the business_vocab dictonary\n",
    "tr_users = tr_df.ReviewerID.apply(lambda x: user_vocab[x]).values\n",
    "tr_items = tr_df.ProductID.apply(lambda x: business_vocab[x]).values\n",
    "val_users = val_df.ReviewerID.apply(lambda x: user_vocab[x] if x in user_vocab else 0).values\n",
    "val_items = val_df.ProductID.apply(lambda x: business_vocab[x] if x in business_vocab else 0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba8f80",
   "metadata": {},
   "source": [
    "### Retrieving ratings in the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ratings = tr_df.Star.values\n",
    "val_ratings = val_df.Star.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b2ebd",
   "metadata": {},
   "source": [
    "### Building the NCF model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_ncf_model(n_users, n_items, embed_size=16, dropout=0.0, output_layer='mlp')\n",
    "# model = build_ncf_combine_model(n_users, n_items, embed_size=16, output_layer='mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd14336",
   "metadata": {},
   "source": [
    "### Training the model using Adam optimizer and mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc550016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(\n",
    "        [tr_users, tr_items], \n",
    "        tr_ratings, \n",
    "        epochs=2, \n",
    "        verbose=1,)\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc8dde",
   "metadata": {},
   "source": [
    "### Evaluating the model on train and validation sets using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model.keras')\n",
    "y_pred = model.predict([tr_users, tr_items])\n",
    "print(\"Train set RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = model.predict([val_users, val_items])\n",
    "print(\"Validation set RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test prediction\n",
    "\n",
    "test_df = pd.read_csv(\"data/prediction.csv\")\n",
    "test_users = test_df.ReviewerID.apply(lambda x: user_vocab[x] if x in user_vocab else 0).values\n",
    "test_items = test_df.ProductID.apply(lambda x: business_vocab[x] if x in business_vocab else 0).values\n",
    "\n",
    "model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "y_pred = model.predict([test_users, test_items])\n",
    "test_df['Star'] = y_pred\n",
    "test_df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Validation prediction\n",
    "\n",
    "model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "y_pred = model.predict([val_users, val_items])\n",
    "val_df['Star'] = y_pred\n",
    "val_df.to_csv('validation_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_submission.h5')\n",
    "y_pred = model.predict([tr_users, tr_items])\n",
    "print(\"Train set RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = model.predict([val_users, val_items])\n",
    "print(\"Validation set RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4721e36",
   "metadata": {},
   "source": [
    "### Parameter-Tuning on MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7dc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_sizes = [16, 50, 64, 128]\n",
    "dropouts = [0.0, 0.15, 0.3]\n",
    "i = 0\n",
    "for embed in embed_sizes:\n",
    "    for dp in dropouts:\n",
    "        model = build_ncf_model(n_users, n_items, embed_size=embed, dropout = dp, output_layer='mlp')\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        history = model.fit(\n",
    "                [tr_users, tr_items], \n",
    "                tr_ratings, \n",
    "                epochs=2, \n",
    "                verbose=0,\n",
    "                callbacks=[ModelCheckpoint(f'models/model{i}.h5')])\n",
    "        \n",
    "        y_pred = model.predict([val_users, val_items])\n",
    "        print(f\"model{i}.h5, Embed Size: {embed}, Dropout: {dp}, Validation set RMSE: \", rmse(y_pred, val_ratings))\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
